{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "## Psuedocode\n",
    "1. policy $\\pi_\\theta$를 이용해서 $m$개의 trajectory $\\{ \\tau^{(1)}, \\tau^{(2)}, ... , \\tau^{(m)} \\}$를 생성한다.\n",
    "   - 각 trajectory는 horizon $H$ 만큼의 길이를 갖는다.\n",
    "   - $i$번째 trajectory $\\tau^{(i)} = (s_0^{(i)}, a_0^{(i)}, ..., s_H^{(i)}, a_H^{(i)}, s_{H+1}^{(i)})$  \n",
    "  \n",
    "2. 모든 trajectory를 이용해서 gradient $\\triangledown_\\theta J(\\theta)$를 추정한다.\n",
    "   - $\\triangledown_{\\theta}J(\\theta) \\approx \\hat{g} := \\frac{1}{m}\\Sigma_{i=1}^m\\Sigma_{t=0}^H \\triangledown_{\\theta}\\log\\pi_{\\theta}(a_t^{(i)} | s_t^{(i)})R(\\tau^{(i)})$  \n",
    "  \n",
    "3. policy의 weights를 업데이트한다.\n",
    "   - $\\theta \\leftarrow \\theta + \\alpha\\hat{g}$\n",
    "  \n",
    "4. 1-3을 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from model import Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11f95c350>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(policy, state, is_train):\n",
    "    # numpy로 된 state를 torch tensor화 한다.\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    \n",
    "    # state를 policy에 넣어 action의 확률을 구한다.\n",
    "    if is_train:\n",
    "        probs = policy(state)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            probs = policy(state)\n",
    "    \n",
    "    # action의 확률값을 이용해 샘플링을 수행하여 액션을 뽑는다.\n",
    "    # torch.distributions의 Categorical을 이용해 샘플링에 대한 최적화 수행 가능\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    \n",
    "    # 해당 action의 log_probs를 저장한다.\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    \n",
    "    # 액션의 값을 리턴한다.\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode(policy, optimizer, gamma, eps):\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    \n",
    "    # policy가 모은 rewards의 값에 할인율 gamma를 적용한다.\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    \n",
    "    # 리턴을 정규화한다.\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    \n",
    "    # -log_prob * reward\n",
    "    # (-)는 Gradient Descent가 아닌 Gradient Ascent이므로\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    # 개별 loss의 총합이 policy_loss가 됨\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "    \n",
    "    return policy, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy, optimizer, gamma, eps, render):\n",
    "    running_reward = 10\n",
    "    \n",
    "    # n번 반복한다 - 여기서는 맨 끝에 CartPole 문제가 풀릴 때까지.\n",
    "    for i_episode in count(1):\n",
    "        \n",
    "        # 환경과 reward를 리셋\n",
    "        state, ep_reward = env.reset(), 0\n",
    "        \n",
    "        # 게임을 플레이한다. 학습시에는 스텝은 10,000번으로 제한을 둔다.\n",
    "        for t in range(1, 10000):\n",
    "            \n",
    "            # 현재 state에 대한 action을 선택한다.\n",
    "            action = select_action(policy, state, is_train=True)\n",
    "            \n",
    "            # 선택한 action을 실행해서 다음 state, reward, done, info를 얻는다.\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # render option이 true인 경우 시각적으로 보여준다.\n",
    "            if render:\n",
    "                env.render()\n",
    "                \n",
    "            # reward를 policy의 rewards에 추가한다.\n",
    "            policy.rewards.append(reward)\n",
    "            \n",
    "            # ep_reward에 더해준다.\n",
    "            ep_reward += reward\n",
    "            \n",
    "            # 게임 오버되면 loop를 종료한다.\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        \n",
    "        # 에피소드가 끝나면 policy를 업데이트한다.\n",
    "        policy, optimizer = finish_episode(policy, optimizer, gamma, eps)\n",
    "        \n",
    "        # 로깅\n",
    "        log_interval = 10\n",
    "        if i_episode % log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            \n",
    "            torch.save(policy.state_dict(), \"solved.pth\")\n",
    "            print(\"model saved.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tLast reward: 36.00\tAverage reward: 13.69\n",
      "Episode 20\tLast reward: 33.00\tAverage reward: 19.32\n",
      "Episode 30\tLast reward: 37.00\tAverage reward: 25.70\n",
      "Episode 40\tLast reward: 60.00\tAverage reward: 40.37\n",
      "Episode 50\tLast reward: 92.00\tAverage reward: 49.16\n",
      "Episode 60\tLast reward: 146.00\tAverage reward: 61.31\n",
      "Episode 70\tLast reward: 104.00\tAverage reward: 87.16\n",
      "Episode 80\tLast reward: 132.00\tAverage reward: 107.66\n",
      "Episode 90\tLast reward: 151.00\tAverage reward: 159.99\n",
      "Episode 100\tLast reward: 191.00\tAverage reward: 146.75\n",
      "Episode 110\tLast reward: 202.00\tAverage reward: 217.45\n",
      "Episode 120\tLast reward: 103.00\tAverage reward: 177.93\n",
      "Episode 130\tLast reward: 128.00\tAverage reward: 138.37\n",
      "Episode 140\tLast reward: 312.00\tAverage reward: 156.50\n",
      "Episode 150\tLast reward: 386.00\tAverage reward: 247.40\n",
      "Episode 160\tLast reward: 500.00\tAverage reward: 333.87\n",
      "Episode 170\tLast reward: 468.00\tAverage reward: 398.93\n",
      "Episode 180\tLast reward: 187.00\tAverage reward: 318.90\n",
      "Episode 190\tLast reward: 126.00\tAverage reward: 246.76\n",
      "Episode 200\tLast reward: 103.00\tAverage reward: 193.66\n",
      "Episode 210\tLast reward: 183.00\tAverage reward: 173.73\n",
      "Episode 220\tLast reward: 130.00\tAverage reward: 163.14\n",
      "Episode 230\tLast reward: 240.00\tAverage reward: 174.60\n",
      "Episode 240\tLast reward: 164.00\tAverage reward: 178.15\n",
      "Episode 250\tLast reward: 216.00\tAverage reward: 176.21\n",
      "Episode 260\tLast reward: 136.00\tAverage reward: 171.89\n",
      "Episode 270\tLast reward: 134.00\tAverage reward: 171.71\n",
      "Episode 280\tLast reward: 113.00\tAverage reward: 167.85\n",
      "Episode 290\tLast reward: 120.00\tAverage reward: 162.53\n",
      "Episode 300\tLast reward: 239.00\tAverage reward: 166.26\n",
      "Episode 310\tLast reward: 312.00\tAverage reward: 190.39\n",
      "Episode 320\tLast reward: 296.00\tAverage reward: 208.70\n",
      "Episode 330\tLast reward: 318.00\tAverage reward: 252.71\n",
      "Episode 340\tLast reward: 500.00\tAverage reward: 320.57\n",
      "Episode 350\tLast reward: 500.00\tAverage reward: 383.91\n",
      "Episode 360\tLast reward: 500.00\tAverage reward: 430.49\n",
      "Episode 370\tLast reward: 500.00\tAverage reward: 437.26\n",
      "Episode 380\tLast reward: 500.00\tAverage reward: 427.36\n",
      "Episode 390\tLast reward: 500.00\tAverage reward: 452.89\n",
      "Episode 400\tLast reward: 500.00\tAverage reward: 471.80\n",
      "Solved! Running reward is now 475.8182660762279 and the last episode runs to 500 time steps!\n",
      "model saved.\n"
     ]
    }
   ],
   "source": [
    "RENDER = False\n",
    "LR = 0.01\n",
    "GAMMA = 0.99\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=LR)\n",
    "EPS = np.finfo(np.float32).eps.item() #small eps value\n",
    "\n",
    "train(policy, optimizer, GAMMA, EPS, RENDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Policy(\n",
       "  (affine1): Linear(in_features=4, out_features=128, bias=True)\n",
       "  (dropout): Dropout(p=0.6, inplace=False)\n",
       "  (affine2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_policy = Policy()\n",
    "path = \"solved.pth\"\n",
    "trained_policy.load_state_dict(torch.load(path, map_location=\"cpu\"))\n",
    "trained_policy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Policy(\n",
       "  (affine1): Linear(in_features=4, out_features=128, bias=True)\n",
       "  (dropout): Dropout(p=0.6, inplace=False)\n",
       "  (affine2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_policy = Policy()\n",
    "random_policy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode reward sum: 500.0\n"
     ]
    }
   ],
   "source": [
    "def play(env, policy):\n",
    "    \n",
    "    state, ep_reward = env.reset(), 0\n",
    "        \n",
    "    # 게임을 플레이한다.\n",
    "    for t in count(1):\n",
    "\n",
    "        # 현재 state에 대한 action을 선택한다.\n",
    "        action = select_action(policy, state, is_train=False)\n",
    "\n",
    "        # 선택한 action을 실행해서 다음 state, reward, done, info를 얻는다.\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        ep_reward += reward\n",
    "\n",
    "        # 플레이를 시각적으로 보여준다.\n",
    "        env.render()\n",
    "            \n",
    "        if done:\n",
    "            print(\"episode reward sum: {}\".format(ep_reward))\n",
    "            break\n",
    "            \n",
    "    env.close()\n",
    "                \n",
    "                \n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(1)\n",
    "play(env, trained_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode reward sum: 21.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.seed(1)\n",
    "play(env, random_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
